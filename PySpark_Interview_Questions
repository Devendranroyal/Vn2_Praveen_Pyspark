# 1-9-2021 questions:

1.) ways to read a file in pysaprk and what are the file extension that pyspark support to read?
There are three ways to read text files into PySpark DataFrame.
--> Using spark.read.text()
--> Using spark.read.csv()
--> Using spark.read.format().load()

2.) ways to write a file in pysaprk and what are the file extension that pyspark support to write?

3.) difference between filter and where?
--> Both 'filter' and 'where' in Spark SQL gives same result. There is no difference between the two.
--> It's just filter is simply the standard Scala name for such a function, and where is for people who prefer SQL

4.) how select is used in pyspark?
--> In PySpark, select() function is used to select single, multiple, column by index, all columns from the list and the nested columns from a DataFrame, 
--> PySpark select() is a transformation function hence it returns a new DataFrame with the selected columns.

5.) what is different types of filter in pyspark?

1.) repartition vs coalesce
2.) hadoop architecture
3.) spark architecture
4.) partition vs bucketing
5.) data skewness 
6.) data shuffling

1.) ways to read a file in pysaprk and what are the file extension that pyspark support to read?
2.) ways to write a file in pysaprk and what are the file extension that pyspark support to write?
3.) difference between filter and where?
4.) how select is used in pyspark?
5.) what is different types of filter in pyspark?
6.) what is Caching and Persistence of DataFrame?

1.) what is rdd?
2.) what is dataframe?
3.) what is fault tolerance?
4.) what is bigdata?
5.) create a udf to reverse a string in pyspark.
6.) create a udf to get len of string.

1.) about yourself?
2.) etl vs elt, which one is better?
3.) datalake vs datawarehouse
4.) fact vs dimention
5.) what is fact?
6.) what is dimention?
7.) how to write etl pipeline?
8.) what is the consideration for etl pipeline
9.) what is kind error you have handled?
10.) what is spark context?
11.) what is satges in saprk?
12.) olap vs oltp
13.) list vs tuple
14.) joins in sql?
15.) acid property?
16.) have you worked in snowflake?
17.) have you handled transaction with comparision of another table?
18.) where is destination?
19.) primary key vs forigien key vs composite key
20.) does forign key have duplcate value
21.) oops in python
22.) inheritance , polymerphism
23.) batch vs streming
24.) rdd, why rdd is lazy evaluation and what is resilent in rdd?
25.) what is transformation and action?
26.) spark driver architecture?
27.) start schema vs snoflake schema
28.) datalake vs datawarehouse
29.) spark sql or pyspark sql
30.) delta lake in databricks is data lake or datawarehouse
--> Delta Lake is a project initiated by Databricks, which is now opensource. 
--> Delta lake is an open-source storage layer that helps you build a data lake comprised of one or more tables in Delta Lake format
--> Delta Lake allows you to operate a multicloud lakehouse architecture that provides data warehousing performance at data lake economics for up to 6x better price/performance for SQL workloads than traditional cloud data warehouses.


# S&P Global - Ankit

Self introduction

1. Spark Architecture
The Spark follows the master-slave architecture. Its cluster consists of a single master and multiple slaves.
The Spark architecture depends upon two abstractions:
	Resilient Distributed Dataset (RDD)
	Directed Acyclic Graph (DAG)
We also have few components in the spark Architecture those are:
	Driver node
	worker node
	Executor
	Cluster manager
	Task
Resilient Distributed Datasets (RDD)
------------------------------
The Resilient Distributed Datasets are the group of data items that can be stored in-memory on worker nodes. Here,
	Resilient: Restore the data on failure.
	Distributed: Data is distributed among different nodes.
	Dataset: Group of data.
	We will learn about RDD later in detail.

Directed Acyclic Graph (DAG)
----------------------------
Directed Acyclic Graph is a finite direct graph that performs a sequence of computations on data. Each node is an RDD partition, and the edge is a transformation on top of data. Here, the graph refers the navigation whereas directed and acyclic refers to how it is done.

Driver Node
-----------
The Driver Program is a process that runs the main() function of the application and creates the SparkContext object. The purpose of SparkContext is to coordinate the spark applications, running as independent sets of processes on a cluster.

To run on a cluster, the SparkContext connects to a different type of cluster managers and then perform the following tasks: -

It acquires executors on nodes in the cluster.
Then, it sends your application code to the executors. Here, the application code can be defined by JAR or Python files passed to the SparkContext.
At last, the SparkContext sends tasks to the executors to run.

Cluster Manager
---------------
The role of the cluster manager is to allocate resources across applications. The Spark is capable enough of running on a large number of clusters.
It consists of various types of cluster managers such as Hadoop YARN, Apache Mesos and Standalone Scheduler.
Here, the Standalone Scheduler is a standalone spark cluster manager that facilitates to install Spark on an empty set of machines.

Worker Node
-----------
The worker node is a slave node
Its role is to run the application code in the cluster.

Executor 
--------
An executor is a process launched for an application on a worker node.
It runs tasks and keeps data in memory or disk storage across them.
It read and write data to the external sources.
Every application contains its executor.

Task
----
A unit of work that will be sent to one executor.

2. What is Spark Driver
A Spark driver (aka an applicationâ€™s driver process) is a JVM process that hosts SparkContext for a Spark application. It is the master node in a Spark application.
It is the cockpit of jobs and tasks execution (using DAGScheduler and Task Scheduler).
It hosts Web UI for the environment.
It splits a Spark application into tasks and schedules them to run on executors.
A driver is where the task scheduler lives and spawns tasks across workers.
A driver coordinates workers and overall execution of tasks.

3. Whom will decide the division of data in the spark driver and worker node
4. how will you filter the records from the particular data frame, like df name is city, He wants the records 
	 We use isin() method for that, here we can left join for better perfomance.
5. If you have 2df and large in size, and you can not join them in your system, then what will be your approach towared that.
		We will 
6. Caching techniques in spark and y we use caching techniques?
7. when we have Squit data(key value format) and specific key is in multiple keys then how will you process the data and we can't delete the duplicate data? (salting techniques)
8. Optimization techniques spark/Hive 
9. How did you connect to the table using Pyspark.
10. Suppose sale table, with few employee columns emp_name, designation, dept, sales. Now i want top3 emp od every dept who did most sale
    df.groupby('dept')
11. Debugging
12. Class method in python
13. Lambda functions.
14. repartion and colease
15. class and static method
16. data skewness

https://www.geeksforgeeks.org/filtering-a-pyspark-dataframe-using-isin-by-exclusion/

#Ramu internal evaluation
1.) About Your Project
2.) What is difficulties you faced in your project
3.) Repartition vs Colesec, which one is better and why
4.) Parquet vs Avro, Which is better and why
6.) RDD vs Dateframes, Which is better and why
7.) Typetest in Spark Scala
8.) Optimized Technics in Spark
--> Data Serialization
--> Data filtering as early as possible
--> Api selection
--> File format selection
--> Parallelism using repartition/coalesce
--> caching/persistance
--> Use of advance variables.
